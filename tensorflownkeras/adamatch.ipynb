{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"adamatch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPB3z31ufDyIwk4I3W2PuSb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"easqifpo1uIC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658377687483,"user_tz":-540,"elapsed":104959,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"23e8ae8a-cafd-49d9-e290-6dd62019cee1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.1 MB 11.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 57.9 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 7.7 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 47.5 MB/s \n","\u001b[K     |████████████████████████████████| 48.3 MB 149 kB/s \n","\u001b[K     |████████████████████████████████| 92 kB 13.1 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 511.7 MB 5.5 kB/s \n","\u001b[K     |████████████████████████████████| 4.6 MB 42.1 MB/s \n","\u001b[K     |████████████████████████████████| 237 kB 57.9 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 43.8 MB/s \n","\u001b[K     |████████████████████████████████| 636 kB 53.7 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 45.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 18.7 MB/s \n","\u001b[K     |████████████████████████████████| 438 kB 41.7 MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official"]},{"cell_type":"code","source":["pip install keras-cv --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvdV-0R86tm2","executionInfo":{"status":"ok","timestamp":1658377691499,"user_tz":-540,"elapsed":4044,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"a77a8bed-7807-48aa-f2e8-a646cbdd87ed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-cv\n","  Downloading keras_cv-0.2.9-py3-none-any.whl (203 kB)\n","\u001b[K     |████████████████████████████████| 203 kB 16.7 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from keras-cv) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-cv) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-cv) (3.0.9)\n","Installing collected packages: keras-cv\n","Successfully installed keras-cv-0.2.9\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.random.set_seed(42)\n","\n","import numpy as np\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from keras_cv.layers import RandAugment\n","\n","import tensorflow_datasets as tfds\n","\n","tfds.disable_progress_bar()"],"metadata":{"id":"JoX7HJlakx68","executionInfo":{"status":"ok","timestamp":1658377694660,"user_tz":-540,"elapsed":3171,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = keras.datasets.mnist.load_data()\n","\n","mnist_x_train = tf.expand_dims(mnist_x_train, -1)\n","mnist_x_test = tf.expand_dims(mnist_x_test, -1)\n","\n","mnist_y_train = tf.one_hot(mnist_y_train, 10).numpy()\n","\n","svhn_train, svhn_test = tfds.load(\n","    \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True\n",")"],"metadata":{"id":"Yy87NlQPEhTX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658378369540,"user_tz":-540,"elapsed":674900,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"f7961ef8-0e61-40ac-be8c-4a2aaab7ecad"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","\u001b[1mDownloading and preparing dataset svhn_cropped/3.0.0 (download: 1.47 GiB, generated: Unknown size, total: 1.47 GiB) to /root/tensorflow_datasets/svhn_cropped/3.0.0...\u001b[0m\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteZPL2LX/svhn_cropped-train.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteZPL2LX/svhn_cropped-test.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteZPL2LX/svhn_cropped-extra.tfrecord\n","\u001b[1mDataset svhn_cropped downloaded and prepared to /root/tensorflow_datasets/svhn_cropped/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"code","source":["RESIZE_TO = 32\n","SOURCE_BATCH_SIZE = 64\n","TARGET_BATCH_SIZE = 3\n","EPOCHS = 10\n","STEPS_PER_EPOCH = len(mnist_x_train) / SOURCE_BATCH_SIZE\n","TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n","\n","\n","AUTO = tf.data.AUTOTUNE\n","LEARNING_RATE = 0.03\n","\n","WEIGHT_DECAY = 0.0005\n","INIT = \"he_normal\"\n","DEPTH = 28\n","WIDTH_MULT = 2"],"metadata":{"id":"f08sE1j9kLvH","executionInfo":{"status":"ok","timestamp":1658378369541,"user_tz":-540,"elapsed":32,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["augmenter = RandAugment(value_range=(0, 255),augmentations_per_image= 2, magnitude=0.5)\n","\n","def weak_augment(image, source=True):\n","  if image.dtype != tf.float32:\n","    image = tf.cast(image, tf.float32)\n","\n","  if source:\n","    image =tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","    image = tf.tile(image, [1, 1, 3])\n","  image = tf.image.random_flip_left_right(image)\n","  image = tf.image.random_crop(image, (RESIZE_TO, RESIZE_TO, 3))\n","  return image\n","\n","def strong_augment(image, source=True):\n","  if image.dtype != tf.float32:\n","    image = tf.cast(image, tf.float32)\n","\n","  if source:\n","    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","    image = tf.tile(image, [1, 1, 3])\n","  image = augmenter(image)\n","  return image"],"metadata":{"id":"y3d7prB0c8wL","executionInfo":{"status":"ok","timestamp":1658378369542,"user_tz":-540,"elapsed":7,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def create_individual_ds(ds, aug_func, source=True):\n","  if source:\n","    batch_size = SOURCE_BATCH_SIZE\n","  else:\n","    batch_size = TARGET_BATCH_SIZE\n","  ds = ds.shuffle(batch_size * 10, seed=42)\n","\n","  if source:\n","    ds = ds.map(lambda x, y: (aug_func(x), y), num_parallel_calls=AUTO)\n","  else:\n","    df = ds.map(lambda x, y: (aug_func(x, False), y), num_parallel_calls=AUTO)\n","\n","  ds = ds.batch(batch_size).prefetch(AUTO)\n","  return ds"],"metadata":{"id":"mfRunCCkvg71","executionInfo":{"status":"ok","timestamp":1658378369542,"user_tz":-540,"elapsed":6,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["source_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n","source_ds_w = create_individual_ds(source_ds, weak_augment)\n","source_ds_s = create_individual_ds(source_ds, strong_augment)\n","final_source_ds = tf.data.Dataset.zip((source_ds_w, source_ds_s))\n","\n","target_ds_w = create_individual_ds(svhn_train, weak_augment, source=False)\n","target_ds_s = create_individual_ds(svhn_train, strong_augment, source=False)\n","final_target_ds = tf.data.Dataset.zip((target_ds_w, target_ds_s))"],"metadata":{"id":"YPj5zAaK5XbR","executionInfo":{"status":"ok","timestamp":1658378373582,"user_tz":-540,"elapsed":4046,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def compute_loss_source(source_labels, logits_source_w, logits_source_s):\n","  loss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\n","  w_loss = loss_func(source_labels, logits_source_w)\n","  s_loss = loss_func(source_labels, logits_source_s)\n","  return w_loss + s_loss\n","\n","def compute_loss_target(target_pseudo_labels_w, logits_target_s, mask):\n","  loss_func = keras.losses.CategoricalCrossentropy(from_logits=True, reduction=\"none\")\n","  target_pseudo_labels_w = tf.stop_gradient(target_pseudo_labels_w)\n","  target_loss = loss_func(target_pseudo_labels_w, logits_target_s)\n","\n","  mask = tf.cast(mask, target_loss.dtype)\n","  target_loss *= mask\n","  return tf.reduce_mean(target_loss, 0)"],"metadata":{"id":"u44Y0Jz07z7H","executionInfo":{"status":"ok","timestamp":1658378374014,"user_tz":-540,"elapsed":440,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class AdaMatch(keras.Model):\n","  def __init__(self, model, total_steps, tau=0.9):\n","      super(AdaMatch, self).__init__()\n","      self.model = model\n","      self.tau = tau \n","      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n","      self.total_steps = total_steps\n","      self.current_step = tf.Variable(0, dtype=\"int64\")\n","\n","  @property\n","  def metrics(self):\n","    return [self.loss_tracker]\n","\n","  def compute_mu(self):\n","    pi = tf.constant(np.pi, dtype=\"float32\")\n","    step = tf.cast(self.current_step, dtype=\"float32\")\n","    return 0.5 - tf.cos(tf.math.minimum(pi, (2 * pi * step) / self.total_steps)) / 2\n","\n","  def train_step(self, data):\n","    source_ds, target_ds = data\n","    (source_w, source_labels), (source_s, _) = source_ds\n","    (\n","        (target_w, _),\n","        (target_s, _),\n","    ) = target_ds\n","\n","    combined_images = tf.concat([source_w, source_s, target_w, target_s], 0)\n","    combined_source = tf.concat([source_w, source_s], 0)\n","\n","    total_source = tf.shape(combined_source)[0]\n","    total_target = tf.shape(tf.concat([target_w, target_s], 0))[0]\n","\n","    with tf.GradientTape() as tape:\n","        combined_logits = self.model(combined_images, training=True)\n","        z_d_prime_source = self.model(\n","            combined_source, training=False\n","        )\n","        z_prime_source = combined_logits[:total_source]\n","\n","        lambd = tf.random.uniform((total_source, 10), 0, 1)\n","        final_source_logits = (lambd * z_prime_source) + (\n","            (1 - lambd) * z_d_prime_source\n","        )\n","\n","        y_hat_source_w = tf.nn.softmax(final_source_logits[: tf.shape(source_w)[0]])\n","\n","        logits_target = combined_logits[total_source:]\n","        logits_target_w = logits_target[: tf.shape(target_w)[0]]\n","        y_hat_target_w = tf.nn.softmax(logits_target_w)\n","\n","        expectation_ratio = tf.reduce_mean(y_hat_source_w) / tf.reduce_mean(\n","            y_hat_target_w\n","        )\n","        y_tilde_target_w = tf.math.l2_normalize(\n","            y_hat_target_w * expectation_ratio, 1\n","        )\n","\n","        row_wise_max = tf.reduce_max(y_hat_source_w, axis=-1)\n","        final_sum = tf.reduce_mean(row_wise_max, 0)\n","        c_tau = self.tau * final_sum\n","        mask = tf.reduce_max(y_tilde_target_w, axis=-1) >= c_tau\n","\n","        source_loss = compute_loss_source(\n","            source_labels,\n","            final_source_logits[: tf.shape(source_w)[0]],\n","            final_source_logits[tf.shape(source_w)[0] :],\n","        )\n","        target_loss = compute_loss_target(\n","            y_tilde_target_w, logits_target[tf.shape(target_w)[0] :], mask\n","        )\n","\n","        t = self.compute_mu()\n","        total_loss = source_loss + (t * target_loss)\n","        self.current_step.assign_add(\n","            1\n","        )  \n","\n","    gradients = tape.gradient(total_loss, self.model.trainable_variables)\n","    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n","\n","    self.loss_tracker.update_state(total_loss)\n","    return {\"loss\": self.loss_tracker.result()}"],"metadata":{"id":"kZlBebslDbSE","executionInfo":{"status":"ok","timestamp":1658378374015,"user_tz":-540,"elapsed":33,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def wide_basic(x, n_input_plane, n_output_plane, stride):\n","  conv_params = [[3, 3, stride, \"same\"], [3, 3, (1, 1), \"same\"]]\n","\n","  n_bottleneck_plane = n_output_plane\n","\n","  for i, v in enumerate(conv_params):\n","    if i == 0:\n","      if n_input_plane != n_output_plane:\n","        x = layers.BatchNormalization()(x)\n","        x = layers.Activation(\"relu\")(x)\n","        convs = x\n","      else:\n","        convs = layers.BatchNormalization()(x)\n","        convs = layers.Activation(\"relu\")(convs)\n","      convs = layers.Conv2D(\n","          n_bottleneck_plane,\n","          (v[0], v[1]),\n","          strides=v[2],\n","          padding=v[3],\n","          kernel_initializer=INIT,\n","          kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","          use_bias=False,\n","      )(convs)\n","    else:\n","      convs = layers.BatchNormalization()(convs)\n","      convs = layers.Activation(\"relu\")(convs)\n","      convs = layers.Conv2D(\n","          n_bottleneck_plane,\n","          (v[0], v[1]),\n","          strides=v[2],\n","          padding=v[3],\n","          kernel_initializer=INIT,\n","          kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","          use_bias=False,\n","      )(convs)\n","\n","  if n_input_plane != n_output_plane:\n","    shortcut = layers.Conv2D(\n","        n_output_plane,\n","        (1, 1),\n","        strides=stride,\n","        padding=\"same\",\n","        kernel_initializer=INIT,\n","        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","        use_bias=False,\n","    )(x)\n","  else:\n","    shortcut = x\n","  \n","  return layers.Add()([convs, shortcut])"],"metadata":{"id":"QTm-VsSN-SnZ","executionInfo":{"status":"ok","timestamp":1658378374016,"user_tz":-540,"elapsed":33,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def block_series(x, n_input_plane, n_output_plane, count, stride):\n","  x = wide_basic(x, n_input_plane, n_output_plane, stride)\n","  for i in range(2, int(count + 1)):\n","    x = wide_basic(x, n_output_plane, n_output_plane, stride=1)\n","  return x\n","\n","def get_network(image_size=32, num_classes=10):\n","  n = (DEPTH - 4) / 6\n","  n_stages = [16, 16*WIDTH_MULT, 32*WIDTH_MULT, 64*WIDTH_MULT]\n","\n","  inputs = keras.Input(shape=(image_size, image_size, 3))\n","  x = layers.Rescaling(scale=1.0 / 255)(inputs)\n","\n","  conv1 = layers.Conv2D(\n","      n_stages[0],\n","      (3, 3),\n","      strides=1,\n","      padding=\"same\",\n","      kernel_initializer=INIT,\n","      kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","      use_bias=False,\n","  )(x)\n","\n","  conv2 = block_series(\n","      conv1,\n","      n_input_plane=n_stages[0],\n","      n_output_plane=n_stages[1],\n","      count=n,\n","      stride=(1, 1),\n","  )\n","\n","  conv3 = block_series(\n","      conv2,\n","      n_input_plane=n_stages[1],\n","      n_output_plane=n_stages[2],\n","      count=n,\n","      stride=(2, 2),\n","  )\n","\n","  conv4 = block_series(\n","      conv3,\n","      n_input_plane=n_stages[2],\n","      n_output_plane=n_stages[3],\n","      count=n,\n","      stride=(2, 2),\n","  )\n","\n","  batch_norm = layers.BatchNormalization()(conv4)\n","  relu = layers.Activation(\"relu\")(batch_norm)\n","\n","  trunk_outputs = layers.GlobalAveragePooling2D()(relu)\n","  outputs = layers.Dense(\n","      num_classes, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n","  )(trunk_outputs)\n","\n","  return keras.Model(inputs, outputs)"],"metadata":{"id":"MLeEQ8sxZSoq","executionInfo":{"status":"ok","timestamp":1658378374017,"user_tz":-540,"elapsed":32,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["wrn_model = get_network()\n","print(f\"Model has {wrn_model.count_params()/1e6} Million parameters.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t478ZMSt-faf","executionInfo":{"status":"ok","timestamp":1658378374760,"user_tz":-540,"elapsed":775,"user":{"displayName":"48","userId":"17678505092892983243"}},"outputId":"8332cea9-2e49-4f3f-ca55-d2d62b515158"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model has 1.471226 Million parameters.\n"]}]},{"cell_type":"code","source":["reduce_lr = keras.optimizers.schedules.CosineDecay(LEARNING_RATE, TOTAL_STEPS, 0.25)\n","optimizer = keras.optimizers.Adam(reduce_lr)\n","\n","adamatch_trainer = AdaMatch(model=wrn_model, total_steps=TOTAL_STEPS)\n","adamatch_trainer.compile(optimizer=optimizer)"],"metadata":{"id":"n5UQTkvR-_Zr","executionInfo":{"status":"ok","timestamp":1658378374761,"user_tz":-540,"elapsed":5,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["total_ds = tf.data.Dataset.zip((final_source_ds, final_target_ds))\n","adamatch_trainer.fit(total_ds, epochs=EPOCHS)"],"metadata":{"id":"O_qgrHQ4AVsq","executionInfo":{"status":"ok","timestamp":1658378682540,"user_tz":-540,"elapsed":394,"user":{"displayName":"48","userId":"17678505092892983243"}}},"execution_count":17,"outputs":[]}]}